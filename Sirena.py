import time
import csv
import logging
import re
from urllib.parse import urljoin, urlparse
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from bs4 import BeautifulSoup

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SirenaSeleniumScraper:
    def __init__(self, headless=True):
        self.base_url = "https://www.sirena.do/"
        self.driver = None
        self.products_data = []
        self.headless = headless
        
    def setup_driver(self):
        """Configurar el driver de Selenium"""
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # Opciones para mejor compatibilidad
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # User agent realista
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Configurar ventana
            chrome_options.add_argument("--window-size=1920,1080")
            
            # Crear driver
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("‚úÖ Driver de Selenium configurado correctamente")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error configurando Selenium: {e}")
            logger.error("üí° Aseg√∫rate de tener ChromeDriver instalado")
            logger.error("üí° Instala con: pip install selenium")
            logger.error("üí° Descarga ChromeDriver desde: https://chromedriver.chromium.org/")
            return False
    
    def get_page_with_js(self, url, wait_seconds=10):
        """Cargar p√°gina y esperar que se renderice el JavaScript"""
        try:
            logger.info(f"üåê Cargando p√°gina: {url}")
            self.driver.get(url)
            
            # Esperar que la p√°gina cargue
            time.sleep(3)
            
            # Intentar detectar cuando el contenido se ha cargado
            try:
                # Esperar por elementos que indiquen que la p√°gina se carg√≥
                WebDriverWait(self.driver, wait_seconds).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                # Esperar un poco m√°s por el contenido din√°mico
                time.sleep(2)
                
                # Hacer scroll para activar lazy loading si existe
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                time.sleep(1)
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(1)
                
            except TimeoutException:
                logger.warning("‚ö†Ô∏è Timeout esperando carga completa, continuando...")
            
            # Obtener el HTML renderizado
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            logger.info("‚úÖ P√°gina cargada y renderizada")
            return soup
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando p√°gina {url}: {e}")
            return None
    
    def analyze_spa_structure(self, soup):
        """Analizar la estructura de la SPA para entender c√≥mo funciona"""
        logger.info("=== AN√ÅLISIS DE APLICACI√ìN WEB (SPA) ===")
        
        # Buscar elementos que se cargan din√°micamente
        dynamic_elements = [
            "div[id*='app']", "div[id*='root']", "div[id*='main']",
            "div[class*='app']", "div[class*='container']", "div[class*='content']",
            "[ng-app]", "[data-react-root]", "[data-vue-root]"
        ]
        
        for selector in dynamic_elements:
            elements = soup.select(selector)
            if elements:
                logger.info(f"‚úÖ Encontrado contenedor SPA: {selector} ({len(elements)} elementos)")
                for elem in elements[:3]:
                    classes = elem.get('class', [])
                    elem_id = elem.get('id', '')
                    logger.info(f"   - ID: {elem_id}, Clases: {classes}")
        
        # Analizar scripts para entender el framework
        scripts = soup.find_all('script')
        frameworks = {
            'React': ['react', 'jsx', 'ReactDOM'],
            'Vue': ['vue', 'v-', 'Vue.js'],
            'Angular': ['angular', 'ng-', '@angular'],
            'Next.js': ['next', '_next'],
            'Nuxt': ['nuxt', '_nuxt'],
            'jQuery': ['jquery', '$']
        }
        
        detected_frameworks = []
        for script in scripts:
            script_content = str(script)
            for framework, indicators in frameworks.items():
                if any(indicator.lower() in script_content.lower() for indicator in indicators):
                    if framework not in detected_frameworks:
                        detected_frameworks.append(framework)
        
        if detected_frameworks:
            logger.info(f"üîç Frameworks detectados: {', '.join(detected_frameworks)}")
        
        # Buscar elementos de navegaci√≥n despu√©s del renderizado
        nav_selectors = [
            'nav', 'header', '.navbar', '.navigation', '.menu',
            '.nav', '.header', '.top-bar', '.main-nav'
        ]
        
        total_nav_elements = 0
        for selector in nav_selectors:
            elements = soup.select(selector)
            if elements:
                total_nav_elements += len(elements)
                logger.info(f"üìã Navegaci√≥n '{selector}': {len(elements)} elementos")
                
                # Mostrar contenido de navegaci√≥n
                for i, elem in enumerate(elements[:2]):
                    links = elem.find_all('a', href=True)
                    if links:
                        logger.info(f"   Nav {i+1}: {len(links)} enlaces")
                        for link in links[:5]:
                            text = link.get_text(strip=True)
                            href = link.get('href')
                            if text and len(text) < 30:
                                logger.info(f"      ‚Üí {text} ({href})")
        
        logger.info(f"üìä Total elementos de navegaci√≥n encontrados: {total_nav_elements}")
        
        # Buscar contenido de productos
        product_indicators = soup.find_all(text=lambda t: t and 
            any(word in str(t).lower() for word in ['producto', 'product', 'precio', 'price', '$', 'rd']))
        
        logger.info(f"üí∞ Indicadores de productos encontrados: {len(product_indicators)}")
        
        # Buscar im√°genes (productos suelen tener muchas im√°genes)
        images = soup.find_all('img')
        logger.info(f"üñºÔ∏è Im√°genes encontradas: {len(images)}")
        
        if images:
            product_images = [img for img in images if 
                any(term in (img.get('src', '') + img.get('alt', '')).lower() 
                    for term in ['product', 'producto', 'item'])]
            logger.info(f"üñºÔ∏è Im√°genes que parecen productos: {len(product_images)}")
        
        logger.info("=== FIN AN√ÅLISIS SPA ===")
    
    def wait_for_dynamic_content(self, timeout=15):
        """Esperar que el contenido din√°mico se cargue"""
        logger.info("‚è≥ Esperando contenido din√°mico...")
        
        # Estrategias para detectar contenido cargado
        strategies = [
            # Esperar por enlaces de navegaci√≥n
            (By.CSS_SELECTOR, "nav a, .navbar a, .menu a", "enlaces de navegaci√≥n"),
            # Esperar por productos
            (By.CSS_SELECTOR, ".product, .item, [data-product]", "elementos de producto"),
            # Esperar por im√°genes de productos
            (By.CSS_SELECTOR, "img[alt*='product'], img[src*='product']", "im√°genes de productos"),
            # Esperar por precios
            (By.XPATH, "//*[contains(text(), '$') or contains(text(), 'RD') or contains(text(), 'precio')]", "elementos con precios")
        ]
        
        for by, selector, description in strategies:
            try:
                logger.info(f"üîç Buscando {description}...")
                elements = WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_all_elements_located((by, selector))
                )
                if elements:
                    logger.info(f"‚úÖ Encontrados {len(elements)} {description}")
                    return True
            except TimeoutException:
                logger.info(f"‚è≥ No se encontraron {description} en {timeout}s")
                continue
        
        logger.warning("‚ö†Ô∏è No se detect√≥ contenido din√°mico espec√≠fico, continuando...")
        return False
    
    def extract_categories_selenium(self):
        """Extraer categor√≠as usando Selenium"""
        soup = self.get_page_with_js(self.base_url, wait_seconds=15)
        if not soup:
            return []
        
        # Analizar estructura SPA
        self.analyze_spa_structure(soup)
        
        # Esperar contenido din√°mico
        self.wait_for_dynamic_content()
        
        # Obtener HTML actualizado despu√©s de esperar
        time.sleep(3)
        updated_html = self.driver.page_source
        soup = BeautifulSoup(updated_html, 'html.parser')
        
        logger.info("üîç Extrayendo categor√≠as del contenido renderizado...")
        
        # Selectores expandidos para SPA
        category_selectors = [
            # Selectores generales
            'nav a', 'header a', '.navbar a', '.menu a',
            '.navigation a', '.nav-link', '.menu-item a',
            # Selectores espec√≠ficos de categor√≠as
            'a[href*="categoria"]', 'a[href*="category"]',
            'a[href*="/c/"]', 'a[href*="/cat/"]',
            'a[href*="productos"]', 'a[href*="products"]',
            # Selectores de dropdown/men√∫
            '.dropdown-item', '.dropdown-menu a',
            '.mega-menu a', '.submenu a',
            # Selectores por contenido
            'a[title*="categoria"]', 'a[aria-label*="categoria"]'
        ]
        
        all_links = []
        
        # Recopilar enlaces con todos los selectores
        for selector in category_selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    logger.info(f"üîó Selector '{selector}': {len(elements)} enlaces")
                    for elem in elements:
                        href = elem.get('href', '')
                        text = elem.get_text(strip=True)
                        if href and text and len(text) > 1:
                            all_links.append({
                                'text': text,
                                'href': href,
                                'selector': selector
                            })
            except Exception as e:
                logger.debug(f"Error con selector {selector}: {e}")
        
        # Si no encuentra enlaces espec√≠ficos, buscar TODOS los enlaces
        if not all_links:
            logger.info("üîç Buscando TODOS los enlaces en la p√°gina renderizada...")
            all_page_links = soup.find_all('a', href=True)
            logger.info(f"üîó Total enlaces encontrados: {len(all_page_links)}")
            
            for link in all_page_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if href and text:
                    all_links.append({
                        'text': text,
                        'href': href,
                        'selector': 'all_links'
                    })
        
        logger.info(f"üìä Total enlaces recopilados: {len(all_links)}")
        
        # Mostrar muestra de enlaces
        logger.info("=== MUESTRA DE ENLACES ENCONTRADOS ===")
        for i, link in enumerate(all_links[:15]):
            logger.info(f"{i+1}. '{link['text'][:40]}' ‚Üí {link['href'][:50]} [{link['selector']}]")
        
        # Filtrar y procesar enlaces
        categories = self.filter_category_links(all_links)
        
        logger.info(f"‚úÖ Categor√≠as finales encontradas: {len(categories)}")
        for i, cat in enumerate(categories[:10]):
            logger.info(f"{i+1}. {cat['name']} ‚Üí {cat['url']}")
        
        return categories
    
    def filter_category_links(self, all_links):
        """Filtrar enlaces para encontrar categor√≠as v√°lidas"""
        categories = []
        
        # Filtros para categor√≠as
        skip_terms = [
            'login', 'registro', 'sign', 'account', 'cuenta', 'perfil',
            'carrito', 'cart', 'checkout', 'pago', 'payment',
            'contacto', 'contact', 'about', 'acerca', 'nosotros',
            'ayuda', 'help', 'support', 'soporte', 'faq',
            'terminos', 'terms', 'privacidad', 'privacy', 'politicas',
            'facebook', 'twitter', 'instagram', 'youtube', 'whatsapp'
        ]
        
        category_indicators = [
            'categoria', 'category', 'departamento', 'seccion',
            'productos', 'products', 'tienda', 'shop', 'store'
        ]
        
        seen_urls = set()
        
        for link_data in all_links:
            text = link_data['text']
            href = link_data['href']
            
            # Filtros b√°sicos
            if (not text or not href or 
                len(text) < 2 or len(text) > 100 or
                href.startswith('#') or 
                href.startswith('javascript:') or
                href.startswith('mailto:') or
                href.startswith('tel:')):
                continue
            
            # Filtrar t√©rminos no deseados
            if any(term in text.lower() for term in skip_terms):
                continue
            
            # Crear URL completa
            if href.startswith('/'):
                full_url = urljoin(self.base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                full_url = urljoin(self.base_url, href)
            
            # Evitar duplicados
            url_key = full_url.lower().rstrip('/')
            if url_key in seen_urls or url_key == self.base_url.rstrip('/').lower():
                continue
            
            seen_urls.add(url_key)
            
            # Priorizar enlaces que parezcan categor√≠as
            is_likely_category = (
                any(indicator in text.lower() for indicator in category_indicators) or
                any(indicator in href.lower() for indicator in category_indicators) or
                (len(text.split()) <= 4 and len(text) > 3)
            )
            
            categories.append({
                'name': text,
                'url': full_url,
                'priority': 1 if is_likely_category else 2
            })
        
        # Ordenar por prioridad y limitar
        categories.sort(key=lambda x: (x['priority'], len(x['name'])))
        return categories[:20]
    
    def extract_products_selenium(self, category_url, category_name):
        """Extraer productos de una categor√≠a usando Selenium"""
        logger.info(f"üõí Extrayendo productos de: {category_name}")
        
        soup = self.get_page_with_js(category_url, wait_seconds=10)
        if not soup:
            return []
        
        # Buscar productos con m√∫ltiples estrategias
        products = []
        
        # Estrategia 1: Selectores espec√≠ficos
        product_selectors = [
            '.product', '.product-item', '.product-card',
            '.item', '.grid-item', '.catalog-item',
            '[data-product]', '[data-product-id]',
            '.card', '.shop-item'
        ]
        
        found_products = False
        for selector in product_selectors:
            elements = soup.select(selector)
            if elements and len(elements) > 2:
                logger.info(f"‚úÖ Productos encontrados con '{selector}': {len(elements)}")
                
                for elem in elements[:30]:  # Limitar para evitar ruido
                    product_data = self.extract_product_info(elem, category_name)
                    if product_data:
                        products.append(product_data)
                
                found_products = True
                break
        
        # Estrategia 2: An√°lisis heur√≠stico si no encuentra productos
        if not found_products:
            logger.info("üîç Usando an√°lisis heur√≠stico para productos...")
            
            # Buscar elementos que contengan im√°genes y precios
            all_divs = soup.find_all('div')
            for div in all_divs:
                has_image = div.find('img')
                has_price_text = div.find(text=lambda t: t and 
                    any(symbol in str(t) for symbol in ['$', 'RD', 'precio', 'price']))
                
                if has_image and has_price_text:
                    product_data = self.extract_product_info(div, category_name)
                    if product_data:
                        products.append(product_data)
        
        logger.info(f"üì¶ Productos extra√≠dos de '{category_name}': {len(products)}")
        return products
    
    def extract_product_info(self, element, category_name):
        """Extraer informaci√≥n de un producto"""
        product = {
            'nombre': '',
            'precio': '',
            'categoria': category_name,
            'imagen': '',
            'enlace': ''
        }
        
        # Extraer nombre
        name_selectors = ['h1', 'h2', 'h3', 'h4', '.product-name', '.name', '.title', 'a']
        for selector in name_selectors:
            name_elem = element.select_one(selector)
            if name_elem:
                name_text = name_elem.get_text(strip=True)
                if name_text and 3 <= len(name_text) <= 150:
                    product['nombre'] = name_text
                    break
        
        # Extraer precio
        price_patterns = [
            r'RD\$\s*[\d,]+\.?\d*',
            r'\$\s*[\d,]+\.?\d*',
            r'[\d,]+\.\d{2}',
            r'[\d,]+\s*pesos?'
        ]
        
        element_text = element.get_text()
        for pattern in price_patterns:
            match = re.search(pattern, element_text, re.IGNORECASE)
            if match:
                product['precio'] = match.group().strip()
                break
        
        # Extraer imagen
        img_elem = element.find('img')
        if img_elem:
            img_src = img_elem.get('src') or img_elem.get('data-src')
            if img_src:
                product['imagen'] = urljoin(self.base_url, img_src)
        
        # Extraer enlace
        link_elem = element.find('a', href=True)
        if link_elem:
            href = link_elem.get('href')
            if href:
                product['enlace'] = urljoin(self.base_url, href)
        
        # Solo devolver si tiene nombre v√°lido
        if product['nombre'] and len(product['nombre']) >= 3:
            return product
        
        return None
    
    def run_selenium_scraping(self):
        """Ejecutar scraping completo con Selenium"""
        logger.info("üöÄ Iniciando scraping con Selenium...")
        
        if not self.setup_driver():
            return False
        
        try:
            # Extraer categor√≠as
            categories = self.extract_categories_selenium()
            if not categories:
                logger.error("‚ùå No se encontraron categor√≠as")
                return False
            
            # Procesar categor√≠as
            for i, category in enumerate(categories, 1):
                logger.info(f"üìÇ [{i}/{len(categories)}] Procesando: {category['name']}")
                
                try:
                    products = self.extract_products_selenium(category['url'], category['name'])
                    self.products_data.extend(products)
                    
                    # Pausa entre categor√≠as
                    time.sleep(3)
                    
                except Exception as e:
                    logger.error(f"‚ùå Error en categor√≠a {category['name']}: {e}")
                    continue
            
            logger.info(f"üéâ Scraping completado. Total productos: {len(self.products_data)}")
            return True
            
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("üîö Driver de Selenium cerrado")
    
    def save_to_csv(self, filename='sirena_productos_selenium.csv'):
        """Guardar productos en CSV"""
        if not self.products_data:
            logger.warning("‚ö†Ô∏è No hay productos para guardar")
            return
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['nombre', 'precio', 'categoria', 'imagen', 'enlace']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for product in self.products_data:
                    writer.writerow(product)
            
            logger.info(f"üíæ Productos guardados en: {filename}")
            print(f"‚úÖ Archivo creado: {filename}")
            print(f"üìä Total productos: {len(self.products_data)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando CSV: {e}")
    
    def print_sample_products(self):
        """Mostrar muestra de productos"""
        if not self.products_data:
            return
        
        print("\nüìã MUESTRA DE PRODUCTOS:")
        print("=" * 60)
        
        for i, product in enumerate(self.products_data[:5], 1):
            print(f"{i}. {product['nombre']}")
            print(f"   üí∞ {product['precio']}")
            print(f"   üìÇ {product['categoria']}")
            print("-" * 40)

def main():
    print("ü§ñ Sirena.do Scraper con Selenium")
    print("=" * 50)
    print("üìã Este scraper maneja contenido JavaScript din√°mico")
    print("‚öôÔ∏è  Requiere: pip install selenium")
    print("üîß Requiere: ChromeDriver instalado")
    print("=" * 50)
    
    # Preguntar si usar modo headless
    try:
        headless_input = input("¬øEjecutar en modo headless? (s/N): ").lower()
        headless = headless_input in ['s', 'y', 'yes', 's√≠']
    except:
        headless = True
    
    scraper = SirenaSeleniumScraper(headless=headless)
    
    if scraper.run_selenium_scraping():
        scraper.print_sample_products()
        scraper.save_to_csv()
    else:
        print("‚ùå Error en el scraping. Revisa los logs.")
    
    print("=" * 50)
    print("‚ú® Proceso completado")

if __name__ == "__main__":
    main()