import time
import csv
import logging
import re
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from bs4 import BeautifulSoup
from collections import Counter

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SirenaSeleniumScraper:
    def __init__(self, headless=True):
        self.base_url = "https://www.sirena.do/"
        self.driver = None
        self.products_data = []
        self.headless = headless
        self.processed_urls = set()
        
    def setup_driver(self):
        """Configurar el driver de Selenium"""
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            chrome_options.add_argument("--window-size=1920,1080")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("‚úÖ Driver de Selenium configurado correctamente")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error configurando Selenium: {e}")
            return False
    
    def get_page_with_js(self, url, wait_seconds=15):
        """Cargar p√°gina y esperar que se renderice el JavaScript"""
        try:
            logger.info(f"üåê Cargando p√°gina: {url}")
            self.driver.get(url)
            
            time.sleep(5)
            
            try:
                WebDriverWait(self.driver, wait_seconds).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                # Scroll para activar lazy loading
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
            except TimeoutException:
                logger.warning("‚ö†Ô∏è Timeout esperando carga completa, continuando...")
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            logger.info("‚úÖ P√°gina cargada y renderizada")
            return soup
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando p√°gina {url}: {e}")
            return None
    
    def extract_all_links(self, soup):
        """Extraer todos los enlaces de la p√°gina"""
        all_links = []
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link.get('href', '').strip()
            text = link.get_text(strip=True)
            
            if not href or not text:
                continue
                
            if href.startswith('/'):
                full_url = urljoin(self.base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                full_url = urljoin(self.base_url, href)
            
            all_links.append({
                'text': text,
                'url': full_url,
                'original_href': href
            })
        
        return all_links
    
    def find_category_links(self, all_links):
        """Encontrar enlaces que sean categor√≠as de productos"""
        skip_terms = [
            'login', 'registro', 'sign', 'account', 'cuenta', 'perfil', 'mi cuenta',
            'carrito', 'cart', 'checkout', 'pago', 'payment', 'comprar',
            'contacto', 'contact', 'about', 'acerca', 'nosotros', 'quienes somos',
            'ayuda', 'help', 'support', 'soporte', 'faq', 'preguntas',
            'terminos', 'terms', 'privacidad', 'privacy', 'politicas',
            'facebook', 'twitter', 'instagram', 'youtube', 'whatsapp', 'redes',
            'sucursales', 'ubicaciones', 'horarios', 'empleo', 'trabajo',
            'newsletter', 'suscribir', 'blog', 'noticias', 'eventos',
            'garantia', 'devolucion', 'envio', 'delivery', 'ver m√°s', 'leer m√°s'
        ]
        
        categories = []
        seen_urls = set()
        
        for link_data in all_links:
            text = link_data['text'].lower()
            url = link_data['url']
            href = link_data['original_href']
            
            # Filtrar enlaces no deseados
            if (len(link_data['text']) < 3 or len(link_data['text']) > 80 or
                any(term in text for term in skip_terms) or
                href.startswith(('#', 'javascript:', 'mailto:', 'tel:'))):
                continue
            
            url_key = url.lower().rstrip('/')
            if url_key in seen_urls or url_key == self.base_url.rstrip('/').lower():
                continue
            seen_urls.add(url_key)
            
            # Identificar posibles categor√≠as
            is_category = (
                '/categoria' in href.lower() or
                '/category' in href.lower() or
                '/c/' in href.lower() or
                '/productos' in href.lower() or
                '/products' in href.lower() or
                '/tienda' in href.lower() or
                '/shop' in href.lower() or
                any(term in text for term in ['electrodomesticos', 'tecnologia', 'hogar', 'muebles', 
                                              'cocina', 'refrigeracion', 'lavado', 'climatizacion',
                                              'audio', 'video', 'celulares', 'computadoras', 'gaming'])
            )
            
            if is_category:
                categories.append({
                    'name': link_data['text'],
                    'url': url
                })
        
        logger.info(f"üìÇ Categor√≠as encontradas: {len(categories)}")
        return categories
    
    def extract_products_from_page(self, soup, category_name, page_url):
        """Extraer productos de una p√°gina usando m√∫ltiples estrategias"""
        products = []
        
        # Estrategias de selecci√≥n ordenadas por efectividad
        selectors_to_try = [
            '.product, .product-item, .product-card, .product-box',
            '.item, .grid-item, .catalog-item, .shop-item',
            '[data-product], [data-product-id], [data-item]',
            '.card, .tile, .box',
            'article, .article',
            '.list-item, .listing-item'
        ]
        
        for selector in selectors_to_try:
            elements = soup.select(selector)
            
            if len(elements) >= 3:  # Si encuentra una cantidad razonable
                logger.info(f"‚úÖ Usando selector '{selector}' - {len(elements)} elementos")
                
                for elem in elements:
                    product_data = self.extract_product_info(elem, category_name, page_url)
                    if product_data:
                        products.append(product_data)
                
                if len(products) >= 3:  # Si extrajo productos v√°lidos
                    break
        
        # Estrategia alternativa: buscar elementos con im√°genes y texto
        if len(products) < 3:
            logger.info("üîç Aplicando extracci√≥n alternativa...")
            products = self.alternative_product_extraction(soup, category_name, page_url)
        
        return products
    
    def extract_product_info(self, element, category_name, page_url):
        """Extraer informaci√≥n espec√≠fica del producto"""
        product = {
            'nombre': '',
            'precio': '',
            'categoria': category_name
        }
        
        # Extraer nombre del producto
        name_selectors = [
            'h1, h2, h3, h4, h5, h6',
            '.product-name, .name, .title, .product-title',
            'a[title]',
            'img[alt]',
            '.text, .description'
        ]
        
        for selector in name_selectors:
            if product['nombre']:
                break
                
            elements = element.select(selector)
            for elem in elements:
                if elem.name == 'img':
                    text = elem.get('alt', '').strip()
                elif elem.name == 'a':
                    text = elem.get('title', '').strip() or elem.get_text(strip=True)
                else:
                    text = elem.get_text(strip=True)
                
                if self.is_valid_product_name(text):
                    product['nombre'] = text
                    break
        
        # Extraer precio
        element_text = element.get_text()
        price = self.extract_price_from_text(element_text)
        if price:
            product['precio'] = price
        
        # Solo retornar si tiene nombre v√°lido
        if product['nombre']:
            return product
        
        return None
    
    def is_valid_product_name(self, text):
        """Validar si un texto es un nombre de producto v√°lido"""
        if not text or len(text) < 5 or len(text) > 200:
            return False
        
        invalid_starts = ['ver', 'more', 'click', 'comprar', 'buy', 'a√±adir', 'add']
        if any(text.lower().startswith(start) for start in invalid_starts):
            return False
        
        if text.isdigit() or text.replace('.', '').replace(',', '').isdigit():
            return False
        
        return True
    
    def extract_price_from_text(self, text):
        """Extraer precio del texto usando patrones"""
        price_patterns = [
            r'RD\$\s*[\d,]+\.?\d*',
            r'\$\s*[\d,]+\.?\d*', 
            r'[\d,]+\.\d{2}\s*RD',
            r'[\d,]+\s*pesos?',
            r'[\d,]{3,}\.\d{2}',
            r'[\d,]{4,}',
            r'Precio:\s*[\d,]+\.?\d*',
            r'Price:\s*[\d,]+\.?\d*'
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                price_text = match.group().strip()
                price_clean = re.sub(r'[^\d,.]', '', price_text)
                if price_clean and len(price_clean) >= 2:
                    return price_text
        
        return ''
    
    def alternative_product_extraction(self, soup, category_name, page_url):
        """Extracci√≥n alternativa basada en heur√≠sticas"""
        products = []
        
        # Buscar divs que contengan im√°genes y posibles precios
        all_containers = soup.find_all(['div', 'article', 'section', 'li'])
        
        for container in all_containers:
            has_image = container.find('img')
            container_text = container.get_text()
            
            # Buscar indicadores de precio
            has_price_indicator = any(indicator in container_text.lower() 
                                    for indicator in ['$', 'rd', 'precio', 'price', 'pesos'])
            
            # Buscar n√∫meros que podr√≠an ser precios
            price_numbers = re.findall(r'[\d,]{3,}', container_text)
            has_potential_price = len(price_numbers) > 0
            
            if has_image and (has_price_indicator or has_potential_price):
                product_data = self.extract_product_info(container, category_name, page_url)
                if product_data and product_data['nombre']:
                    products.append(product_data)
        
        return products
    
    def process_pagination(self, base_url, category_name, max_pages=2):
        """Procesar paginaci√≥n de una categor√≠a"""
        all_products = []
        
        for page_num in range(1, max_pages + 1):
            # Construir URL de p√°gina
            if '?' in base_url:
                page_url = f"{base_url}&page={page_num}"
            else:
                page_url = f"{base_url}?page={page_num}"
            
            logger.info(f"üìÑ Procesando p√°gina {page_num} de {category_name}")
            
            soup = self.get_page_with_js(page_url, wait_seconds=10)
            if not soup:
                break
            
            products = self.extract_products_from_page(soup, category_name, page_url)
            
            if products:
                all_products.extend(products)
                logger.info(f"‚úÖ P√°gina {page_num}: {len(products)} productos")
            else:
                logger.info(f"‚ö†Ô∏è No se encontraron productos en p√°gina {page_num}")
                break
            
            time.sleep(2)  # Pausa entre p√°ginas
        
        return all_products
    
    def process_category(self, category_url, category_name):
        """Procesar una categor√≠a completa con paginaci√≥n"""
        logger.info(f"üìÇ Procesando categor√≠a: {category_name}")
        
        # Procesar hasta 2 p√°ginas
        all_products = self.process_pagination(category_url, category_name, max_pages=2)
        
        # Si no encontr√≥ productos, intentar buscar subcategor√≠as
        if len(all_products) < 5:
            logger.info(f"üîç Buscando subcategor√≠as en: {category_name}")
            
            soup = self.get_page_with_js(category_url, wait_seconds=10)
            if soup:
                all_links = self.extract_all_links(soup)
                subcategories = self.find_subcategories(all_links, category_url)
                
                for subcat in subcategories:
                    if subcat['url'] not in self.processed_urls:
                        self.processed_urls.add(subcat['url'])
                        
                        subcat_products = self.process_pagination(
                            subcat['url'], 
                            f"{category_name} > {subcat['name']}", 
                            max_pages=2
                        )
                        all_products.extend(subcat_products)
                        time.sleep(2)
        
        return all_products
    
    def find_subcategories(self, all_links, parent_url):
        """Encontrar subcategor√≠as dentro de una categor√≠a"""
        subcategories = []
        parent_path = urlparse(parent_url).path
        
        for link_data in all_links:
            url = link_data['url']
            text = link_data['text']
            path = urlparse(url).path
            
            if url == parent_url:
                continue
            
            # Criterios para subcategor√≠as
            is_subcategory = (
                (parent_path in path and path != parent_path and len(path) > len(parent_path)) or
                (len(text.split()) <= 4 and len(text) >= 3 and not text.isdigit())
            )
            
            if is_subcategory:
                subcategories.append({
                    'name': text,
                    'url': url
                })
        
        # Eliminar duplicados
        seen = set()
        unique_subcats = []
        for subcat in subcategories:
            if subcat['url'] not in seen:
                seen.add(subcat['url'])
                unique_subcats.append(subcat)
        
        return unique_subcats[:8]  # M√°ximo 8 subcategor√≠as
    
    def run_selenium_scraping(self):
        """Ejecutar scraping completo"""
        logger.info("üöÄ Iniciando scraping de Sirena.do...")
        
        if not self.setup_driver():
            return False
        
        try:
            # Cargar p√°gina principal
            soup = self.get_page_with_js(self.base_url, wait_seconds=20)
            if not soup:
                logger.error("‚ùå No se pudo cargar la p√°gina principal")
                return False
            
            # Extraer enlaces y encontrar categor√≠as
            all_links = self.extract_all_links(soup)
            categories = self.find_category_links(all_links)
            
            if not categories:
                logger.error("‚ùå No se encontraron categor√≠as")
                return False
            
            logger.info(f"üìÇ Procesando {len(categories)} categor√≠as...")
            
            # Procesar cada categor√≠a
            for i, category in enumerate(categories, 1):
                logger.info(f"üîÑ [{i}/{len(categories)}] {category['name']}")
                
                try:
                    if category['url'] not in self.processed_urls:
                        self.processed_urls.add(category['url'])
                        
                        products = self.process_category(category['url'], category['name'])
                        
                        if products:
                            self.products_data.extend(products)
                            logger.info(f"‚úÖ {category['name']}: {len(products)} productos")
                        
                        time.sleep(3)  # Pausa entre categor√≠as
                    
                except Exception as e:
                    logger.error(f"‚ùå Error en {category['name']}: {e}")
                    continue
            
            logger.info(f"üéâ Scraping completado. Total: {len(self.products_data)} productos")
            return True
            
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("üîö Driver cerrado")
    
    def save_to_csv(self, filename='sirena_productos.csv'):
        """Guardar productos en CSV eliminando duplicados"""
        if not self.products_data:
            logger.warning("‚ö†Ô∏è No hay productos para guardar")
            return
        
        try:
            # Eliminar duplicados
            unique_products = []
            seen = set()
            
            for product in self.products_data:
                key = (product['nombre'].lower().strip(), product['categoria'].lower().strip())
                if key not in seen and product['nombre'].strip():
                    seen.add(key)
                    unique_products.append(product)
            
            logger.info(f"üßπ Productos √∫nicos: {len(unique_products)}")
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['nombre', 'precio', 'categoria']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for product in unique_products:
                    writer.writerow(product)
            
            logger.info(f"üíæ Guardado en: {filename}")
            print(f"‚úÖ Archivo creado: {filename}")
            print(f"üìä Productos √∫nicos: {len(unique_products)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando CSV: {e}")
    
    def print_sample_products(self):
        """Mostrar muestra de productos"""
        if not self.products_data:
            print("‚ùå No hay productos para mostrar")
            return
        
        print("\nüìã MUESTRA DE PRODUCTOS EXTRA√çDOS:")
        print("=" * 80)
        
        # Mostrar primeros 10 productos
        for i, product in enumerate(self.products_data[:10], 1):
            print(f"{i}. {product['nombre']}")
            print(f"   üí∞ Precio: {product['precio'] or 'No disponible'}")
            print(f"   üìÇ Categor√≠a: {product['categoria']}")
            print("-" * 60)
        
        # Estad√≠sticas por categor√≠a
        category_counts = Counter(p['categoria'] for p in self.products_data)
        
        print(f"\nüìä PRODUCTOS POR CATEGOR√çA:")
        print("=" * 50)
        for category, count in category_counts.most_common(10):
            print(f"üìÇ {category}: {count} productos")

def main():
    print("ü§ñ Sirena.do Scraper Mejorado")
    print("=" * 60)
    print("‚úÖ Caracter√≠sticas:")
    print("   ‚Ä¢ Extracci√≥n completa de categor√≠as")
    print("   ‚Ä¢ M√°ximo 2 p√°ginas por categor√≠a")
    print("   ‚Ä¢ Productos reales con nombre, precio y categor√≠a")
    print("   ‚Ä¢ Sin l√≠mites artificiales")
    print("=" * 60)
    
    try:
        headless_input = input("¬øEjecutar sin ventana visible? (s/N): ").lower()
        headless = headless_input in ['s', 'y', 'yes', 's√≠']
    except:
        headless = True
    
    scraper = SirenaSeleniumScraper(headless=headless)
    
    if scraper.run_selenium_scraping():
        scraper.print_sample_products()
        scraper.save_to_csv()
        print(f"\nüéâ ¬°SCRAPING COMPLETADO!")
    else:
        print("‚ùå Error en el scraping")
    
    print("=" * 60)

if __name__ == "__main__":
    main()