import requests
import csv
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from collections import defaultdict
import hashlib
import urllib3
from urllib3.exceptions import InsecureRequestWarning

# Deshabilitar warnings de SSL
urllib3.disable_warnings(InsecureRequestWarning)

def obtener_pagina(url, timeout=30, reintentos=3):
    """Obtener contenido de una p√°gina web"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Connection': 'keep-alive',
        'Accept-Encoding': 'gzip, deflate, br'
    }
    
    for intento in range(reintentos):
        try:
            print(f"Obteniendo: {url[:60]}...")
            response = requests.get(url, headers=headers, timeout=timeout, verify=False)
            response.raise_for_status()
            print(f"‚úì P√°gina obtenida ({response.status_code})")
            return response.text
        except Exception as e:
            print(f"Error intento {intento + 1}: {e}")
            if intento < reintentos - 1:
                time.sleep(5)
    
    print(f"‚ùå Error despu√©s de {reintentos} intentos")
    return None

def normalizar_texto(texto):
    """Normalizar texto para comparaci√≥n (quitar espacios, acentos, may√∫sculas)"""
    if not texto:
        return ""
    
    # Convertir a min√∫sculas y quitar espacios extra
    texto = re.sub(r'\s+', ' ', texto.lower().strip())
    
    # Quitar acentos b√°sicos
    replacements = {
        '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
        '√±': 'n', '√º': 'u'
    }
    for old, new in replacements.items():
        texto = texto.replace(old, new)
    
    return texto

def normalizar_precio(precio):
    """Normalizar precio para comparaci√≥n"""
    if not precio or precio == "Sin precio":
        return ""
    
    # Extraer solo n√∫meros y puntos/comas
    precio_limpio = re.sub(r'[^\d.,]', '', precio)
    # Normalizar separadores decimales
    precio_limpio = precio_limpio.replace(',', '.')
    
    return precio_limpio

def generar_hash_producto(nombre, precio):
    """Generar hash √∫nico basado en nombre y precio normalizados"""
    nombre_norm = normalizar_texto(nombre)
    precio_norm = normalizar_precio(precio)
    
    # Crear string √∫nico
    texto_unico = f"{nombre_norm}|{precio_norm}"
    
    # Generar hash
    return hashlib.md5(texto_unico.encode('utf-8')).hexdigest()

def productos_son_similares(prod1, prod2, umbral_similitud=0.85):
    """Verificar si dos productos son similares usando diferentes criterios"""
    
    # Criterio 1: Hash exacto
    hash1 = generar_hash_producto(prod1['nombre'], prod1['precio'])
    hash2 = generar_hash_producto(prod2['nombre'], prod2['precio'])
    
    if hash1 == hash2:
        return True
    
    # Criterio 2: Nombres muy similares
    nombre1_norm = normalizar_texto(prod1['nombre'])
    nombre2_norm = normalizar_texto(prod2['nombre'])
    
    # Verificar si un nombre est√° contenido en el otro
    if nombre1_norm in nombre2_norm or nombre2_norm in nombre1_norm:
        # Si los nombres son similares, verificar precios
        precio1_norm = normalizar_precio(prod1['precio'])
        precio2_norm = normalizar_precio(prod2['precio'])
        
        if precio1_norm == precio2_norm:
            return True
    
    # Criterio 3: Similitud de Jaccard para nombres
    palabras1 = set(nombre1_norm.split())
    palabras2 = set(nombre2_norm.split())
    
    if palabras1 and palabras2:
        interseccion = len(palabras1.intersection(palabras2))
        union = len(palabras1.union(palabras2))
        similitud = interseccion / union if union > 0 else 0
        
        if similitud >= umbral_similitud:
            precio1_norm = normalizar_precio(prod1['precio'])
            precio2_norm = normalizar_precio(prod2['precio'])
            
            if precio1_norm == precio2_norm:
                return True
    
    return False

def eliminar_duplicados_avanzado(productos):
    """Eliminar duplicados usando m√∫ltiples criterios"""
    print(f"\nüîç ELIMINANDO DUPLICADOS...")
    print(f"Productos originales: {len(productos)}")
    
    productos_unicos = []
    productos_procesados = set()
    
    for i, producto_actual in enumerate(productos):
        
        # Generar hash √∫nico
        hash_actual = generar_hash_producto(producto_actual['Nombre'], producto_actual['Precio'])
        
        # Si ya procesamos este hash exacto, saltar
        if hash_actual in productos_procesados:
            continue
        
        # Verificar similitud con productos ya agregados
        es_duplicado = False
        for producto_unico in productos_unicos:
            if productos_son_similares(
                {'nombre': producto_actual['Nombre'], 'precio': producto_actual['Precio']},
                {'nombre': producto_unico['Nombre'], 'precio': producto_unico['Precio']}
            ):
                es_duplicado = True
                # Combinar categor√≠as si es duplicado
                if producto_actual['Categoria'] not in producto_unico['Categorias']:
                    producto_unico['Categorias'].append(producto_actual['Categoria'])
                break
        
        if not es_duplicado:
            # Crear nuevo producto √∫nico con lista de categor√≠as
            producto_unico = producto_actual.copy()
            producto_unico['Categorias'] = [producto_actual['Categoria']]
            productos_unicos.append(producto_unico)
            productos_procesados.add(hash_actual)
    
    print(f"Productos √∫nicos: {len(productos_unicos)}")
    print(f"Duplicados eliminados: {len(productos) - len(productos_unicos)}")
    
    return productos_unicos

def es_categoria_valida(url, texto):
    """Determinar si es una categor√≠a v√°lida de supermercado - Super Bravo (MEJORADO)"""
    texto_lower = texto.lower().strip()
    url_lower = url.lower()
    
    # Excluir elementos de navegaci√≥n y p√°ginas no deseadas
    excluir = [
        'ver todo', 'ver mas', 'mi cuenta', 'mi carrito', 'mis listas', 
        'mis ordenes', 'cerrar sesi√≥n', 'iniciar sesi√≥n', 'crear cuenta', 
        'nuestras tiendas', 'pol√≠ticas', 'retiro en tienda', 'ayuda', 'contacto',
        'inicio', 'soporte', 'trabaja con nosotros', 'sucursales', 'home',
        'javascript:', '#', 'mailto:', 'tel:', 'login', 'register', 'cart', 
        'checkout', 'search', 'buscar', 'facebook', 'instagram', 'twitter', 
        'youtube', 'whatsapp', 'terminos', 'privacidad', 'cookies',
        'nosotros', 'quienes somos', 'historia', 'mision', 'vision',
        'empleo', 'bolsa de trabajo', 'rrhh', 'recursos humanos'
    ]
    
    # Verificar exclusiones
    for termino in excluir:
        if termino in texto_lower or termino in url_lower:
            return False
    
    # Debe tener texto v√°lido
    if len(texto.strip()) < 2 or len(texto.strip()) > 80:
        return False
    
    # Categor√≠as v√°lidas de supermercado Rep√∫blica Dominicana (AMPLIADO)
    categorias_validas = [
        # Carnes y prote√≠nas
        'carne', 'res', 'pollo', 'cerdo', 'pescado', 'mariscos', 'embutidos',
        'jam√≥n', 'salami', 'chorizo', 'salchicha', 'pavo', 'cordero',
        
        # L√°cteos
        'leche', 'queso', 'yogurt', 'l√°cteos', 'huevos', 'mantequilla',
        'crema', 'nata', 'reques√≥n',
        
        # Frutas y vegetales
        'fruta', 'vegetal', 'verdura', 'hortalizas', 'vegetales', 'frutas',
        'tomate', 'lechuga', 'cebolla', 'zanahoria', 'platano', 'mango',
        'aguacate', 'naranja', 'lim√≥n', 'papa', 'yuca', '√±ame',
        
        # Panader√≠a y cereales
        'pan', 'panader√≠a', 'cereales', 'arroz', 'pasta', 'granos',
        'avena', 'quinoa', 'trigo', 'ma√≠z', 'habichuela', 'frijol',
        'lentejas', 'garbanzo',
        
        # Bebidas
        'bebida', 'agua', 'jugo', 'caf√©', 't√©', 'vino', 'cerveza', 'licores',
        'refresco', 'soda', 'energizante', 'isot√≥nico', 'leche de coco',
        
        # Limpieza y hogar
        'limpieza', 'detergente', 'jab√≥n', 'hogar', 'aseo', 'lavander√≠a',
        'suavizante', 'cloro', 'desinfectante', 'papel higi√©nico',
        'servilleta', 'pa√±uelo', 'toalla', 'esponja',
        
        # Cuidado personal
        'shampoo', 'cuidado personal', 'higiene', 'pa√±al', 'farmacia',
        'desodorante', 'perfume', 'colonia', 'crema', 'loci√≥n',
        'pasta dental', 'cepillo', 'maquillaje', 'protector solar',
        
        # Condimentos y especias
        'sal', 'az√∫car', 'aceite', 'condimento', 'especias', 'salsa',
        'vinagre', 'mayonesa', 'ketchup', 'mostaza', 'ajo', 'ceboll√≠n',
        'cilantro', 'perejil', 'or√©gano', 'comino',
        
        # Conservas y enlatados
        'conserva', 'enlatado', 'mermelada', 'congelado', 'helado',
        'at√∫n', 'sardina', 'frijoles', 'ma√≠z', 'salsa de tomate',
        
        # Mascotas
        'mascota', 'gato', 'perro', 'alimento', 'comida para mascota',
        
        # Despensa general
        'despensa', 'abarrotes', 'snacks', 'dulces', 'chocolate',
        'galleta', 'caramelo', 'chicle', 'nuez', 'almendra',
        
        # Electr√≥nicos y bazar
        'electr√≥nico', 'electrodom√©stico', 'bazar', 'juguete', 'ropa', 'textil',
        'bater√≠a', 'cargador', 'cable', 'aud√≠fono',
        
        # T√©rminos espec√≠ficos de URLs
        'categoria', 'category', 'departamento', 'seccion', 'productos'
    ]
    
    # Verificar si contiene t√©rminos de supermercado
    for termino in categorias_validas:
        if termino in texto_lower:
            return True
    
    # Tambi√©n verificar la URL por patrones espec√≠ficos
    patrones_url_valida = [
        r'/categoria/', r'/category/', r'/dept/', r'/department/',
        r'/seccion/', r'/productos/', r'/product-category/',
        r'/c/', r'/cat/', r'/departamento/'
    ]
    
    for patron in patrones_url_valida:
        if re.search(patron, url_lower):
            return True
    
    return False

def encontrar_categorias(soup, base_url):
    """Encontrar categor√≠as de productos v√°lidas - Super Bravo (MEJORADO)"""
    categorias = set()
    
    print("üîç Buscando categor√≠as con selectores espec√≠ficos...")
    
    # Selectores espec√≠ficos para Super Bravo (m√°s completos)
    selectores = [
        # Men√∫s principales y navegaci√≥n
        '.main-menu a[href]', '.navbar a[href]', '.nav-menu a[href]',
        '.primary-navigation a[href]', '.main-navigation a[href]',
        
        # Navegaci√≥n de categor√≠as
        '.category-menu a[href]', '.categories a[href]', '.cat-menu a[href]',
        '.product-categories a[href]', '.department-menu a[href]',
        
        # Enlaces en header y navigation
        'nav a[href]', 'header a[href]', '.header a[href]',
        '.navigation a[href]', '.menu a[href]', '.nav a[href]',
        
        # Listas de categor√≠as
        'ul li a[href]', '.category-list a[href]', '.dept-list a[href]',
        
        # Dropdowns y submen√∫s
        '.dropdown-menu a[href]', '.submenu a[href]', '.sub-menu a[href]',
        '.dropdown a[href]', '.menu-item a[href]',
        
        # Mega men√∫s
        '.mega-menu a[href]', '.megamenu a[href]', '.mega-dropdown a[href]',
        
        # Sidebar y categor√≠as laterales
        '.sidebar a[href]', '.sidebar-menu a[href]', '.left-menu a[href]',
        '.category-sidebar a[href]', '.filter-menu a[href]',
        
        # Enlaces espec√≠ficos de productos/categor√≠as
        'a[href*="categoria"]', 'a[href*="category"]', 'a[href*="dept"]',
        'a[href*="department"]', 'a[href*="productos"]', 'a[href*="seccion"]',
        
        # Cualquier enlace (como √∫ltimo recurso)
        'a[href]'
    ]
    
    categorias_encontradas_por_selector = {}
    
    for selector in selectores:
        try:
            enlaces = soup.select(selector)
            categorias_selector = set()
            
            for enlace in enlaces:
                href = enlace.get('href', '').strip()
                texto = enlace.get_text().strip()
                
                if href and href not in ['#', '/', 'javascript:void(0)', '']:
                    # Construir URL completa
                    if href.startswith('http'):
                        url_completa = href
                    else:
                        url_completa = urljoin(base_url, href)
                    
                    # Verificar que la URL pertenezca al dominio
                    if 'superbravo.com.do' in url_completa and es_categoria_valida(url_completa, texto):
                        categoria_tuple = (url_completa, texto)
                        categorias.add(categoria_tuple)
                        categorias_selector.add(categoria_tuple)
            
            if categorias_selector:
                categorias_encontradas_por_selector[selector] = len(categorias_selector)
                print(f"  {selector}: {len(categorias_selector)} categor√≠as")
                
        except Exception as e:
            continue
    
    print(f"\nüìä Resumen de categor√≠as encontradas:")
    for selector, cantidad in sorted(categorias_encontradas_por_selector.items(), 
                                   key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {selector}: {cantidad} categor√≠as")
    
    return list(categorias)

def buscar_categorias_adicionales(soup, base_url):
    """Buscar categor√≠as adicionales en elementos espec√≠ficos del sitio"""
    categorias_adicionales = set()
    
    print("üîç Buscando categor√≠as adicionales...")
    
    # Buscar en scripts JSON-LD o datos estructurados
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            import json
            data = json.loads(script.string)
            # Buscar categor√≠as en datos estructurados
            if isinstance(data, dict) and 'category' in str(data).lower():
                print("  Encontrados datos estructurados con categor√≠as")
        except:
            pass
    
    # Buscar en breadcrumbs
    breadcrumbs = soup.select('.breadcrumb a, .breadcrumbs a, .navigation-path a')
    for crumb in breadcrumbs:
        href = crumb.get('href', '').strip()
        texto = crumb.get_text().strip()
        if href and es_categoria_valida(href, texto):
            url_completa = urljoin(base_url, href)
            if 'superbravo.com.do' in url_completa:
                categorias_adicionales.add((url_completa, texto))
    
    # Buscar en mapas del sitio o sitemaps
    sitemap_links = soup.select('a[href*="sitemap"], a[href*="mapa"]')
    for link in sitemap_links[:2]:  # Limitar a 2 para no sobrecargar
        href = link.get('href')
        if href:
            sitemap_url = urljoin(base_url, href)
            print(f"  Revisando sitemap: {sitemap_url}")
            # Aqu√≠ podr√≠as implementar l√≥gica para procesar sitemaps
    
    return list(categorias_adicionales)

def extraer_productos_pagina(soup):
    """Extraer productos de una p√°gina - Super Bravo"""
    productos = []
    
    # Selectores espec√≠ficos para Super Bravo (adaptables)
    selectores_productos = [
        # Selectores comunes de productos
        '.product-item', '.product', '.item-product', '.producto',
        '.product-card', '.product-box', '.item-box',
        # Selectores de grids y listas
        'div[class*="product"]', 'li[class*="product"]',
        '.grid-item', '.list-item', '.item', '.card',
        # Selectores de art√≠culos
        'article', '.catalog-item', '.shop-item',
        # Selectores gen√©ricos
        '.product-wrap', '.product-container', '.item-wrap'
    ]
    
    items_encontrados = []
    for selector in selectores_productos:
        try:
            items = soup.select(selector)
            if len(items) > len(items_encontrados):
                items_encontrados = items
                print(f"Mejor selector encontrado: {selector} ({len(items)} items)")
        except:
            continue
    
    print(f"Encontrados {len(items_encontrados)} elementos de productos")
    
    for item in items_encontrados:
        try:
            nombre = extraer_nombre_producto(item)
            precio = extraer_precio_producto(item)
            
            if nombre and len(nombre.strip()) > 2:
                productos.append({
                    'nombre': nombre,
                    'precio': precio
                })
        except:
            continue
    
    return productos

def extraer_nombre_producto(item):
    """Extraer nombre del producto - Super Bravo"""
    selectores_nombre = [
        # Selectores espec√≠ficos de nombres
        'a.product-item-link', '.product-name a', '.product-title',
        '.product-name', '.item-name', '.product-title',
        # T√≠tulos y encabezados
        'h1', 'h2', 'h3', 'h4', 'h5',
        '.name', '.title', '.product-info h3', '.product-info h4',
        # Enlaces con t√≠tulo
        'a[title]', 'a.product-link',
        # Selectores alternativos
        '.product-description', '.item-title'
    ]
    
    for selector in selectores_nombre:
        try:
            elemento = item.select_one(selector)
            if elemento:
                texto = elemento.get_text().strip()
                if texto and len(texto) > 2 and len(texto) < 150:
                    return texto
                
                # Intentar atributos
                for attr in ['title', 'alt', 'data-name', 'data-title']:
                    valor = elemento.get(attr, '').strip()
                    if valor and len(valor) > 2 and len(valor) < 150:
                        return valor
        except:
            continue
    
    return "Sin nombre"

def extraer_precio_producto(item):
    """Extraer precio del producto - Super Bravo"""
    selectores_precio = [
        # Selectores espec√≠ficos de precios
        '.price', '.precio', '.cost', '.amount',
        'span[class*="price"]', 'div[class*="price"]',
        '.product-price', '.item-price', '.price-current',
        '.price-now', '.sale-price', '.regular-price',
        # Selectores con moneda dominicana
        'span[class*="pesos"]', 'span[class*="rd"]',
        '.currency', '.money'
    ]
    
    for selector in selectores_precio:
        try:
            elemento = item.select_one(selector)
            if elemento:
                precio_texto = elemento.get_text().strip()
                # Buscar patrones de precio dominicano (RD$ o $)
                if precio_texto and ('RD$' in precio_texto or '$' in precio_texto or 
                                   re.search(r'\d+[.,]\d+', precio_texto)):
                    return precio_texto
        except:
            continue
    
    # Buscar patrones de precio en todo el texto del item
    texto_completo = item.get_text()
    patrones_precio = [
        r'RD\$\s*\d+[.,]?\d*', r'\$\s*\d+[.,]?\d*',
        r'\d+[.,]\d+\s*RD\$', r'\d+[.,]\d+\s*\$',
        r'\d{1,6}[.,]\d{2}', r'\d+\.\d{2}', r'\d+,\d{2}'
    ]
    
    for patron in patrones_precio:
        match = re.search(patron, texto_completo)
        if match:
            return match.group().strip()
    
    return "Sin precio"

def procesar_categoria(url_categoria, nombre_categoria):
    """Procesar todos los productos de una categor√≠a - Super Bravo"""
    print(f"\n{'='*50}")
    print(f"PROCESANDO CATEGOR√çA: {nombre_categoria}")
    print(f"URL: {url_categoria}")
    print(f"{'='*50}")
    
    productos_categoria = []
    
    # Obtener p√°gina de la categor√≠a
    html = obtener_pagina(url_categoria)
    if not html:
        print("‚ùå No se pudo obtener la p√°gina")
        return []
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Extraer productos de esta p√°gina
    productos = extraer_productos_pagina(soup)
    
    for producto in productos:
        productos_categoria.append({
            'Nombre': producto['nombre'],
            'Precio': producto['precio'],
            'Categoria': nombre_categoria,
            'URL_Categoria': url_categoria
        })
    
    print(f"‚úì {len(productos_categoria)} productos extra√≠dos de '{nombre_categoria}'")
    
    print(f"‚úì TOTAL EN '{nombre_categoria}': {len(productos_categoria)} productos")
    return productos_categoria

def main():
    base_url = 'https://www.superbravo.com.do/'
    todos_productos = []
    
    print("üöÄ INICIANDO SCRAPING DE SUPER BRAVO (VERSI√ìN MEJORADA)")
    print("=" * 60)
    
    # Obtener p√°gina principal
    print("Obteniendo p√°gina principal...")
    html_principal = obtener_pagina(base_url)
    
    if not html_principal:
        print("‚ùå No se pudo obtener la p√°gina principal")
        return
    
    soup_principal = BeautifulSoup(html_principal, 'html.parser')
    
    # Encontrar categor√≠as principales
    print("\nBuscando categor√≠as de productos...")
    categorias = encontrar_categorias(soup_principal, base_url)
    
    # Buscar categor√≠as adicionales
    categorias_adicionales = buscar_categorias_adicionales(soup_principal, base_url)
    categorias.extend(categorias_adicionales)
    
    if not categorias:
        print("‚ùå No se encontraron categor√≠as v√°lidas")
        return
    
    # Remover duplicados de categor√≠as
    categorias_unicas = list(set(categorias))
    print(f"\n‚úì {len(categorias_unicas)} categor√≠as √∫nicas encontradas:")
    for i, (url, nombre) in enumerate(categorias_unicas[:25], 1):  # Mostrar las primeras 25
        print(f"  {i:2d}. {nombre} -> {url}")
    
    if len(categorias_unicas) > 25:
        print(f"  ... y {len(categorias_unicas) - 25} m√°s")
    
    print(f"\nCOMENZANDO EXTRACCI√ìN DE PRODUCTOS...")
    
    # Procesar cada categor√≠a (limitar a las primeras 20 para prueba)
    categorias_a_procesar = categorias_unicas[:20]
    
    for i, (url_categoria, nombre_categoria) in enumerate(categorias_a_procesar, 1):
        try:
            print(f"\n[{i}/{len(categorias_a_procesar)}] Procesando: {nombre_categoria}")
            
            productos_categoria = procesar_categoria(url_categoria, nombre_categoria)
            
            if productos_categoria:
                todos_productos.extend(productos_categoria)
                print(f"‚úì {len(productos_categoria)} productos agregados")
            
            time.sleep(2)  # Pausa entre categor√≠as
            
        except Exception as e:
            print(f"‚ùå Error procesando {nombre_categoria}: {e}")
            continue
    
    # ELIMINAR DUPLICADOS
    if todos_productos:
        productos_unicos = eliminar_duplicados_avanzado(todos_productos)
        
        # Preparar datos finales con categor√≠as combinadas
        productos_finales = []
        for producto in productos_unicos:
            productos_finales.append({
                'Nombre': producto['Nombre'],
                'Precio': producto['Precio'],
                'Categorias': '; '.join(producto['Categorias']),
                'URL_Categoria': producto['URL_Categoria']
            })
        
        # Guardar resultados finales
        timestamp = int(time.time())
        archivo_final = f'inventario_superbravo_{timestamp}.csv'
        
        with open(archivo_final, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['Nombre', 'Precio', 'Categorias', 'URL_Categoria'])
            writer.writeheader()
            writer.writerows(productos_finales)
        
        print(f'\nüéâ SCRAPING COMPLETADO')
        print(f'‚úì {len(productos_finales)} productos √∫nicos guardados en {archivo_final}')
        
        # Resumen por categor√≠a
        resumen = defaultdict(int)
        for producto in productos_unicos:
            for categoria in producto['Categorias']:
                resumen[categoria] += 1
        
        print(f"\nüìä RESUMEN POR CATEGOR√çA:")
        for categoria, cantidad in sorted(resumen.items(), key=lambda x: x[1], reverse=True)[:15]:
            print(f"   {categoria}: {cantidad} productos")
            
    else:
        print('\n‚ùå No se extrajo ning√∫n producto')

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚èπ Proceso interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()