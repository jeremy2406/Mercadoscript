import requests
import csv
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from collections import defaultdict

def obtener_pagina(url, timeout=60, reintentos=5):
    """Obtener contenido de una p√°gina web con reintentos agresivos"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'no-cache'
    }
    
    for intento in range(reintentos):
        try:
            print(f"üîÑ Intento {intento + 1}/{reintentos} para: {url[:80]}...")
            session = requests.Session()
            response = session.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            print(f"‚úî P√°gina obtenida exitosamente (Status: {response.status_code}, Tama√±o: {len(response.text)} chars)")
            return response.text
        except requests.exceptions.Timeout:
            print(f"‚è∞ Timeout en intento {intento + 1}")
            if intento < reintentos - 1:
                time.sleep(10)
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error en intento {intento + 1}: {e}")
            if intento < reintentos - 1:
                time.sleep(10)
        except Exception as e:
            print(f"üö® Error inesperado en intento {intento + 1}: {e}")
            if intento < reintentos - 1:
                time.sleep(10)
    
    print(f"‚ùå FALL√ì despu√©s de {reintentos} intentos")
    return None

def encontrar_todos_los_enlaces(soup, base_url):
    """Encontrar TODOS los enlaces posibles que puedan ser categor√≠as"""
    todos_enlaces = set()
    
    # Encontrar todos los enlaces en la p√°gina
    enlaces = soup.find_all('a', href=True)
    print(f"üîç Analizando {len(enlaces)} enlaces en total...")
    
    palabras_clave = [
        'categoria', 'category', 'departamento', 'seccion', 'productos',
        'product', 'item', 'catalogo', 'tienda', 'shop', 'store',
        'carne', 'lacteo', 'bebida', 'limpieza', 'hogar', 'personal',
        'fruta', 'verdura', 'panaderia', 'congelado', 'dulce', 'snack'
    ]
    
    for enlace in enlaces:
        href = enlace.get('href', '').strip()
        texto = enlace.get_text().strip()
        
        if not href or href in ['#', '/', 'javascript:void(0)']:
            continue
            
        url_completa = urljoin(base_url, href)
        
        # Filtrar por URL que contenga palabras clave
        url_lower = url_completa.lower()
        texto_lower = texto.lower()
        
        es_categoria = False
        
        # Verificar si es una categor√≠a por URL
        for palabra in palabras_clave:
            if palabra in url_lower:
                es_categoria = True
                break
        
        # Verificar si es una categor√≠a por texto del enlace
        if not es_categoria and texto and len(texto) > 2 and len(texto) < 100:
            for palabra in palabras_clave:
                if palabra in texto_lower:
                    es_categoria = True
                    break
        
        # Tambi√©n incluir enlaces que tengan cierta estructura
        if not es_categoria:
            if re.search(r'/[a-zA-Z-]+/[a-zA-Z-]+', href) or 'id=' in href or 'cat=' in href:
                es_categoria = True
        
        if es_categoria and url_completa != base_url:
            nombre_categoria = texto if texto else href.split('/')[-1]
            todos_enlaces.add((url_completa, nombre_categoria))
    
    print(f"‚úî Encontrados {len(todos_enlaces)} enlaces potenciales de categor√≠as")
    return list(todos_enlaces)

def buscar_productos_exhaustivo(soup):
    """Buscar productos usando TODOS los selectores posibles"""
    selectores_productos = [
        # Selectores espec√≠ficos comunes
        '.product-item', '.product', '.item-product', '.producto',
        'div[class*="product"]', 'li[class*="product"]',
        '.grid-item', '.product-card', '.item', '.card',
        '[data-product-id]', '[data-product]', '[data-item]',
        
        # Selectores de listas
        'ul.products li', 'ol.products li', '.products .item',
        '.product-list .item', '.items-list .item',
        
        # Selectores de grillas
        '.grid .item', '.row .col', '.flex-item',
        '.catalog-item', '.shop-item', '.store-item',
        
        # Selectores m√°s gen√©ricos
        'article', '.article', 'div[itemtype*="Product"]',
        '[class*="item"]', '[class*="card"]'
    ]
    
    productos_encontrados = []
    
    for selector in selectores_productos:
        try:
            items = soup.select(selector)
            if items and len(items) > len(productos_encontrados):
                productos_encontrados = items
                print(f"‚úî Mejor resultado: {len(items)} productos con selector: {selector}")
        except Exception as e:
            continue
    
    # Si no encontramos productos con selectores espec√≠ficos, buscar por patrones
    if len(productos_encontrados) < 5:
        print("üîç Buscando productos por patrones en el HTML...")
        
        # Buscar divs que contengan informaci√≥n de precio
        divs_con_precio = soup.find_all('div', text=re.compile(r'\$|precio|price|‚Ç¨|‚Ç°|‚Çµ', re.I))
        divs_padre_precio = []
        for div in divs_con_precio:
            parent = div.parent
            if parent:
                divs_padre_precio.append(parent)
        
        if divs_padre_precio:
            productos_encontrados = divs_padre_precio
            print(f"‚úî Encontrados {len(productos_encontrados)} productos por patr√≥n de precio")
    
    return productos_encontrados

def extraer_info_producto_exhaustivo(item):
    """Extraer informaci√≥n del producto de manera exhaustiva"""
    nombre = 'Nombre no disponible'
    precio = 'Precio no disponible'
    
    # Selectores para nombres (m√°s exhaustivos)
    selectores_nombre = [
        'a.product-item-link', '.product-name a', '.product-title',
        'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
        '.name a', '.title a', 'a[title]',
        '.product-name', '.name', '.title', '.titulo',
        '[class*="name"]', '[class*="title"]', '[class*="titulo"]',
        'a', 'span.name', 'div.name', 'p.name'
    ]
    
    # Selectores para precios (m√°s exhaustivos)
    selectores_precio = [
        'span.price', '.price', '.precio', '.cost', '.amount',
        '[class*="price"]', '[class*="precio"]', '[class*="cost"]',
        'span[class*="$"]', 'div[class*="$"]',
        '.currency', '.money', '.value', '.valor'
    ]
    
    # Buscar nombre
    for selector in selectores_nombre:
        try:
            elemento = item.select_one(selector)
            if elemento:
                # Intentar obtener texto de diferentes maneras
                texto_candidatos = [
                    elemento.get_text().strip(),
                    elemento.get('title', '').strip(),
                    elemento.get('alt', '').strip(),
                    elemento.get('data-name', '').strip()
                ]
                
                for texto in texto_candidatos:
                    if texto and len(texto) > 2 and len(texto) < 200:
                        nombre = texto
                        break
                
                if nombre != 'Nombre no disponible':
                    break
        except:
            continue
    
    # Buscar precio
    for selector in selectores_precio:
        try:
            elemento = item.select_one(selector)
            if elemento:
                precio_texto = elemento.get_text().strip()
                if precio_texto and ('$' in precio_texto or '‚Ç°' in precio_texto or 
                                   re.search(r'\d+[.,]\d+', precio_texto)):
                    precio = precio_texto
                    break
        except:
            continue
    
    # Si no encontr√≥ precio, buscar n√∫meros que parezcan precios
    if precio == 'Precio no disponible':
        texto_completo = item.get_text()
        patrones_precio = [
            r'\$\s*\d+[.,]?\d*',
            r'‚Ç°\s*\d+[.,]?\d*',
            r'\d+[.,]\d+\s*‚Ç°',
            r'\d+[.,]\d+\s*\$',
            r'precio[:\s]*\d+[.,]?\d*',
            r'\d{1,6}[.,]\d{2}'
        ]
        
        for patron in patrones_precio:
            match = re.search(patron, texto_completo, re.I)
            if match:
                precio = match.group()
                break
    
    return nombre, precio

def buscar_paginacion(soup, base_url):
    """Buscar enlaces de paginaci√≥n para obtener m√°s productos"""
    enlaces_paginacion = set()
    
    selectores_paginacion = [
        '.pagination a', '.pager a', '.page-numbers a',
        'a[href*="page"]', 'a[href*="p="]', 'a[href*="pagina"]',
        '.next a', '.siguiente a', 'a.next', 'a.siguiente',
        '[class*="pagination"] a', '[class*="pager"] a'
    ]
    
    for selector in selectores_paginacion:
        try:
            elementos = soup.select(selector)
            for elem in elementos:
                href = elem.get('href')
                if href:
                    url_completa = urljoin(base_url, href)
                    enlaces_paginacion.add(url_completa)
        except:
            continue
    
    return list(enlaces_paginacion)

def procesar_categoria_completa(url_categoria, nombre_categoria, base_url):
    """Procesar una categor√≠a completamente incluyendo todas sus p√°ginas"""
    print(f"\nüè™ PROCESANDO CATEGOR√çA COMPLETA: {nombre_categoria}")
    print(f"üîó URL: {url_categoria}")
    
    productos_categoria = []
    paginas_procesadas = set()
    paginas_por_procesar = [url_categoria]
    
    while paginas_por_procesar:
        url_actual = paginas_por_procesar.pop(0)
        
        if url_actual in paginas_procesadas:
            continue
            
        paginas_procesadas.add(url_actual)
        print(f"\nüìÑ Procesando p√°gina: {url_actual}")
        
        html_pagina = obtener_pagina(url_actual)
        if not html_pagina:
            continue
        
        soup = BeautifulSoup(html_pagina, 'html.parser')
        
        # Buscar productos en esta p√°gina
        items = buscar_productos_exhaustivo(soup)
        productos_en_pagina = 0
        
        if items:
            print(f"üõç Encontrados {len(items)} productos en esta p√°gina")
            
            for item in items:
                try:
                    nombre, precio = extraer_info_producto_exhaustivo(item)
                    
                    if nombre != 'Nombre no disponible' and len(nombre.strip()) > 2:
                        productos_categoria.append({
                            'Nombre': nombre,
                            'Precio': precio,
                            'Categor√≠a': nombre_categoria,
                            'URL_Categoria': url_categoria
                        })
                        productos_en_pagina += 1
                
                except Exception as e:
                    continue
            
            print(f"‚úÖ {productos_en_pagina} productos v√°lidos extra√≠dos de esta p√°gina")
        else:
            print("‚ö† No se encontraron productos en esta p√°gina")
        
        # Buscar m√°s p√°ginas (paginaci√≥n)
        enlaces_paginacion = buscar_paginacion(soup, base_url)
        for enlace in enlaces_paginacion:
            if enlace not in paginas_procesadas and enlace not in paginas_por_procesar:
                paginas_por_procesar.append(enlace)
                print(f"üìã Agregada p√°gina para procesar: {enlace}")
        
        # Pausa entre p√°ginas
        time.sleep(3)
        
        # Limitar para evitar bucles infinitos
        if len(paginas_procesadas) > 20:
            print("‚ö† L√≠mite de p√°ginas alcanzado para esta categor√≠a")
            break
    
    print(f"üéØ TOTAL EN CATEGOR√çA '{nombre_categoria}': {len(productos_categoria)} productos")
    return productos_categoria

def main():
    base_url = 'https://supermercadosnacional.com/'
    todos_productos = []
    
    print("üöÄ INICIANDO SCRAPING EXHAUSTIVO")
    print("=" * 80)
    
    # Obtener p√°gina principal
    print("üìÑ Obteniendo p√°gina principal...")
    html_principal = obtener_pagina(base_url)
    
    if not html_principal:
        print("‚ùå No se pudo obtener la p√°gina principal")
        return
    
    soup_principal = BeautifulSoup(html_principal, 'html.parser')
    
    # Encontrar TODOS los enlaces posibles
    print("\nüîç BUSCANDO TODOS LOS ENLACES POSIBLES...")
    todas_categorias = encontrar_todos_los_enlaces(soup_principal, base_url)
    
    if not todas_categorias:
        print("‚ùå No se encontraron categor√≠as")
        return
    
    print(f"\nüìä ENCONTRADAS {len(todas_categorias)} CATEGOR√çAS POTENCIALES")
    print("\nüìã Lista de categor√≠as a procesar:")
    for i, (url, nombre) in enumerate(todas_categorias, 1):
        print(f"  {i:3d}. {nombre[:60]}")
    
    # Procesar cada categor√≠a de manera exhaustiva
    contador_categorias = 0
    contador_productos_total = 0
    
    for i, (url_categoria, nombre_categoria) in enumerate(todas_categorias, 1):
        try:
            print(f"\n{'='*20} CATEGOR√çA {i}/{len(todas_categorias)} {'='*20}")
            
            productos_categoria = procesar_categoria_completa(url_categoria, nombre_categoria, base_url)
            
            if productos_categoria:
                todos_productos.extend(productos_categoria)
                contador_categorias += 1
                contador_productos_total += len(productos_categoria)
                
                print(f"‚úÖ Categor√≠a completada: {len(productos_categoria)} productos")
            else:
                print(f"‚ö† Sin productos en: {nombre_categoria}")
            
            # Guardar progreso cada 10 categor√≠as
            if i % 10 == 0:
                timestamp = int(time.time())
                archivo_progreso = f'progreso_inventario_{timestamp}.csv'
                
                with open(archivo_progreso, mode='w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=['Nombre', 'Precio', 'Categor√≠a', 'URL_Categoria'])
                    writer.writeheader()
                    for producto in todos_productos:
                        writer.writerow(producto)
                
                print(f"üíæ PROGRESO GUARDADO: {len(todos_productos)} productos en {archivo_progreso}")
                
        except Exception as e:
            print(f"‚ùå Error procesando {nombre_categoria}: {e}")
            continue
    
    # Guardar resultados finales
    if todos_productos:
        timestamp = int(time.time())
        nombre_archivo = f'inventario_nacional_{timestamp}.csv'
        
        with open(nombre_archivo, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['Nombre', 'Precio', 'Categor√≠a', 'URL_Categoria'])
            writer.writeheader()
            for producto in todos_productos:
                writer.writerow(producto)
        
        print(f'\nüéâ SCRAPING COMPLETADO')
        print(f'‚úÖ {len(todos_productos)} productos guardados en {nombre_archivo}')
        
        # Resumen por categor√≠a
        categorias_resumen = defaultdict(int)
        for producto in todos_productos:
            categorias_resumen[producto['Categor√≠a']] += 1
        
        print(f"\nüìä RESUMEN FINAL:")
        print(f"   ‚Ä¢ Categor√≠as procesadas: {contador_categorias}")
        print(f"   ‚Ä¢ Total de productos: {len(todos_productos)}")
        print(f"\nüìã Productos por categor√≠a:")
        
        for categoria, cantidad in sorted(categorias_resumen.items(), key=lambda x: x[1], reverse=True):
            print(f"   ‚Ä¢ {categoria[:50]}: {cantidad} productos")
            
    else:
        print('\n‚ùó No se extrajo ning√∫n producto.')

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚èπ Script interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error no controlado: {e}")
        import traceback
        print(traceback.format_exc())