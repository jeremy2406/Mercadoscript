import time
import csv
import logging
import re
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from collections import Counter

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JumboCompleteScraper:
    def __init__(self, headless=True, target_products=2000):
        self.base_url = "https://jumbo.com.do/"
        self.driver = None
        self.products_data = []
        self.headless = headless
        self.processed_urls = set()
        self.target_products = target_products
        self.unique_products = set()  # Para evitar duplicados
        
        # Categor√≠as principales objetivo (m√°s espec√≠ficas para Rep√∫blica Dominicana)
        self.target_categories = {
            "Supermercado": ["supermercado", "mercado", "alimentos", "comida", "groceries", "abarrotes", "food"],
            "Belleza y Salud": ["belleza", "salud", "cuidado personal", "higiene", "beauty", "health", "cosmetic"],
            "Hogar": ["hogar", "cocina", "limpieza", "muebles", "home", "casa", "furniture"],
            "Electrodom√©sticos": ["electrodom√©sticos", "electrodomestico", "electro", "appliances", "electronics"],
            "Ferreter√≠a": ["ferreter√≠a", "ferreteria", "herramientas", "tools", "hardware", "construccion"],
            "Deportes": ["deportes", "fitness", "ejercicio", "sports", "deporte"],
            "Beb√©s": ["beb√©s", "bebes", "ni√±os", "ni√±as", "baby", "kids", "infantil"],
            "Oficina": ["oficina", "√∫tiles", "escolares", "office", "school", "papeleria"],
            "Jugueter√≠a": ["juguetes", "juegos", "toys", "games", "jugueteria"],
            "Tecnolog√≠a": ["tecnologia", "tech", "computacion", "celulares", "phones"],
            "Ropa": ["ropa", "vestir", "clothing", "fashion", "textil"],
            "Autom√≥vil": ["auto", "carro", "vehiculo", "automotive", "car"]
        }
        
    def setup_driver(self):
        """Configurar el driver de Selenium optimizado para JavaScript"""
        try:
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # Opciones optimizadas para sitios JavaScript
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Optimizaciones para velocidad manteniendo funcionalidad
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-logging")
            chrome_options.add_argument("--disable-dev-tools")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # Timeout optimizado
            self.driver.implicitly_wait(3)
            logger.info("‚úÖ Driver configurado correctamente")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error configurando Selenium: {e}")
            return False
    
    def get_page_with_js_wait(self, url, max_retries=2):
        """Cargar p√°gina esperando a que JavaScript termine de cargar"""
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                
                # Esperar a que el body est√© presente
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, 'body'))
                )
                
                # Esperar un poco para JavaScript
                time.sleep(2)
                
                # Scroll para activar lazy loading
                self.driver.execute_script("window.scrollTo(0, 500);")
                time.sleep(1)
                self.driver.execute_script("window.scrollTo(0, 0);")
                
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                text_content = soup.get_text(strip=True)
                
                if len(text_content) > 300:
                    return soup
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Intento {attempt + 1} fallido: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)
        
        return None

    def find_main_categories(self):
        """Buscar categor√≠as principales con estrategias m√∫ltiples"""
        soup = self.get_page_with_js_wait(self.base_url)
        if not soup:
            return []
        
        categories = []
        
        # ESTRATEGIA 1: Navegaci√≥n principal
        nav_selectors = [
            'nav a', 'header nav a', '.navbar a', '.nav a', '.navigation a',
            '.main-menu a', '.primary-menu a', '.menu-principal a',
            '.header-menu a', '.top-menu a', '.main-nav a'
        ]
        
        for selector in nav_selectors:
            links = soup.select(selector)
            logger.info(f"üîç Selector '{selector}': {len(links)} enlaces")
            
            for link in links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                if self.is_potential_category(text, href):
                    full_url = urljoin(self.base_url, href)
                    categories.append({
                        'name': self.normalize_category_name(text),
                        'url': full_url,
                        'parent': None,
                        'level': 'main'
                    })
        
        # ESTRATEGIA 2: B√∫squeda por palabras clave en todos los enlaces
        if len(categories) < 8:
            all_links = soup.find_all('a', href=True)
            logger.info(f"üîç Analizando {len(all_links)} enlaces por palabras clave")
            
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                if self.is_target_category(text) and len(text) > 3 and len(text) < 50:
                    full_url = urljoin(self.base_url, href)
                    categories.append({
                        'name': self.normalize_category_name(text),
                        'url': full_url,
                        'parent': None,
                        'level': 'main'
                    })
        
        # Limpiar duplicados
        unique_categories = self.clean_categories(categories)
        
        logger.info(f"üìä CATEGOR√çAS PRINCIPALES ENCONTRADAS: {len(unique_categories)}")
        for cat in unique_categories:
            logger.info(f"   üìÇ {cat['name']} -> {cat['url']}")
        
        return unique_categories
    
    def find_subcategories(self, main_category, max_subcategories=15):
        """Buscar subcategor√≠as de una categor√≠a principal"""
        soup = self.get_page_with_js_wait(main_category['url'])
        if not soup:
            return []
        
        subcategories = []
        
        # ESTRATEGIA 1: Buscar en men√∫s laterales y filtros
        sidebar_selectors = [
            '.sidebar a', '.filters a', '.categories a', 
            '.menu-lateral a', '.category-nav a', '.subcategory a',
            '[class*="filter"] a', '[class*="category"] a',
            '[class*="submenu"] a', '[class*="subcategory"] a'
        ]
        
        for selector in sidebar_selectors:
            links = soup.select(selector)
            
            for link in links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                if self.is_valid_subcategory(text, href, main_category):
                    full_url = urljoin(self.base_url, href)
                    subcategories.append({
                        'name': text,
                        'url': full_url,
                        'parent': main_category['name'],
                        'level': 'sub'
                    })
        
        # ESTRATEGIA 2: Buscar enlaces dentro de secciones de categor√≠a
        category_sections = soup.find_all(['div', 'section'], class_=lambda x: x and any(
            keyword in x.lower() for keyword in ['category', 'filter', 'menu', 'nav']
        ))
        
        for section in category_sections:
            links = section.find_all('a', href=True)
            for link in links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                if self.is_valid_subcategory(text, href, main_category):
                    full_url = urljoin(self.base_url, href)
                    subcategories.append({
                        'name': text,
                        'url': full_url,
                        'parent': main_category['name'],
                        'level': 'sub'
                    })
        
        # Limpiar y limitar subcategor√≠as
        cleaned_subs = self.clean_categories(subcategories)[:max_subcategories]
        
        logger.info(f"   üìÅ Encontradas {len(cleaned_subs)} subcategor√≠as para {main_category['name']}")
        for sub in cleaned_subs:
            logger.info(f"      ‚Ü≥ {sub['name']}")
        
        return cleaned_subs
    
    def find_pagination_links(self, soup, current_url):
        """Buscar enlaces de paginaci√≥n en la p√°gina actual"""
        pagination_links = []
        
        # Selectores comunes para paginaci√≥n
        pagination_selectors = [
            '.pagination a', '.pager a', '.page-nav a',
            '[class*="page"] a', '[class*="next"] a',
            '.paginacion a', '.paginas a'
        ]
        
        for selector in pagination_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True).lower()
                
                # Buscar enlaces "siguiente" o n√∫meros de p√°gina
                if href and (text.isdigit() or 'next' in text or 'siguiente' in text or 'm√°s' in text):
                    full_url = urljoin(self.base_url, href)
                    if full_url != current_url and full_url not in pagination_links:
                        pagination_links.append(full_url)
        
        # Tambi√©n buscar par√°metros de p√°gina en la URL
        if '?' in current_url:
            base_url = current_url.split('?')[0]
            for page_num in range(2, 6):  # P√°ginas 2-5
                page_url = f"{base_url}?page={page_num}"
                if page_url not in pagination_links:
                    pagination_links.append(page_url)
        
        return pagination_links[:4]  # M√°ximo 4 p√°ginas adicionales
    
    def extract_products_complete(self, category, max_pages=5):
        """Extraer productos de una categor√≠a con paginaci√≥n"""
        all_products = []
        pages_to_process = [category['url']]
        
        # Obtener primera p√°gina y buscar paginaci√≥n
        soup = self.get_page_with_js_wait(category['url'])
        if soup:
            # Extraer productos de la primera p√°gina
            products = self.extract_products_from_page(soup, category)
            all_products.extend(products)
            
            # Buscar m√°s p√°ginas
            pagination_links = self.find_pagination_links(soup, category['url'])
            pages_to_process.extend(pagination_links[:max_pages-1])
        
        # Procesar p√°ginas adicionales
        for i, page_url in enumerate(pages_to_process[1:], 2):
            if len(all_products) >= 50:  # L√≠mite por categor√≠a
                break
                
            logger.info(f"      üìÑ P√°gina {i}: {page_url}")
            page_soup = self.get_page_with_js_wait(page_url)
            if page_soup:
                page_products = self.extract_products_from_page(page_soup, category)
                if page_products:
                    all_products.extend(page_products)
                else:
                    break  # Si no hay productos, probablemente no hay m√°s p√°ginas
            
            time.sleep(1)  # Pausa entre p√°ginas
        
        logger.info(f"üì¶ Total extra√≠do de {category['name']}: {len(all_products)} productos")
        return all_products
    
    def extract_products_from_page(self, soup, category):
        """Extraer productos de una p√°gina espec√≠fica"""
        products = []
        
        # Selectores optimizados para productos
        product_selectors = [
            '[class*="product-item"]', '[class*="product-card"]', 
            '[class*="item-product"]', '[class*="product"]',
            '[data-product]', '[data-item]',
            '.card', '.tile', '[class*="card"]'
        ]
        
        for selector in product_selectors:
            containers = soup.select(selector)
            
            if containers:
                logger.info(f"      üîç Usando selector '{selector}': {len(containers)} elementos")
                
                for container in containers:
                    product = self.extract_product_data(container, category)
                    if product:
                        # Evitar duplicados usando nombre + categor√≠a como clave
                        product_key = f"{product['nombre'].lower()}_{product['categoria']}"
                        if product_key not in self.unique_products:
                            self.unique_products.add(product_key)
                            products.append(product)
                
                if products:  # Si encontramos productos, no probar m√°s selectores
                    break
        
        # M√©todo alternativo si no encontramos productos
        if not products:
            products = self.alternative_product_extraction(soup, category)
        
        return products[:40]  # M√°ximo 40 productos por p√°gina
    
    def alternative_product_extraction(self, soup, category):
        """M√©todo alternativo para extraer productos"""
        products = []
        
        # Buscar divs que contengan imagen + t√≠tulo
        all_divs = soup.find_all('div')[:200]  # Limitar b√∫squeda
        
        for div in all_divs:
            if len(products) >= 30:
                break
                
            img = div.find('img')
            title_elem = div.find(['h1', 'h2', 'h3', 'h4', 'h5', 'span', 'a'])
            
            if img and title_elem:
                product = self.extract_product_data(div, category)
                if product:
                    product_key = f"{product['nombre'].lower()}_{product['categoria']}"
                    if product_key not in self.unique_products:
                        self.unique_products.add(product_key)
                        products.append(product)
        
        return products
    
    def extract_product_data(self, container, category):
        """Extraer datos del producto desde el contenedor"""
        try:
            # Extraer nombre
            name = self.extract_product_name(container)
            if not name or not self.is_valid_product_name(name):
                return None
            
            # Extraer precio
            price = self.extract_product_price(container)
            
            # Construir categor√≠a completa
            full_category = category['name']
            if category.get('parent'):
                full_category = f"{category['parent']} > {category['name']}"
            
            return {
                'nombre': name[:120],
                'precio': price or 'Precio no disponible',
                'categoria': full_category
            }
            
        except Exception:
            return None
    
    def extract_product_name(self, container):
        """Extraer nombre del producto"""
        name_selectors = [
            'h1', 'h2', 'h3', 'h4', 'h5',
            '[class*="name"]', '[class*="title"]', '[class*="product-name"]',
            '[class*="item-name"]', '[data-name]', 'a[title]'
        ]
        
        for selector in name_selectors:
            element = container.select_one(selector)
            if element:
                text = element.get_text(strip=True)
                if text and 3 < len(text) < 150:
                    return text
        
        # M√©todo alternativo con atributos de imagen
        img = container.find('img')
        if img:
            for attr in ['alt', 'title', 'data-name']:
                value = img.get(attr, '').strip()
                if value and 3 < len(value) < 150:
                    return value
        
        return None
    
    def extract_product_price(self, container):
        """Extraer precio del producto"""
        price_selectors = [
            '[class*="price"]', '[class*="precio"]', '[class*="cost"]',
            '[data-price]', '.currency', '[class*="amount"]'
        ]
        
        for selector in price_selectors:
            elements = container.select(selector)
            for element in elements:
                price_text = element.get_text(strip=True)
                parsed_price = self.parse_price(price_text)
                if parsed_price:
                    return parsed_price
        
        return None
    
    def parse_price(self, price_text):
        """Parsear y formatear precio"""
        try:
            clean_text = re.sub(r'[^\d.,]', '', price_text)
            
            price_patterns = [
                r'(\d{1,3}(?:[,.]\d{3})*(?:[.,]\d{2})?)',
                r'(\d+[.,]\d{2})',
                r'(\d+)'
            ]
            
            for pattern in price_patterns:
                match = re.search(pattern, clean_text)
                if match:
                    num_str = match.group(1)
                    
                    if ',' in num_str and '.' in num_str:
                        if num_str.rindex(',') > num_str.rindex('.'):
                            num_str = num_str.replace('.', '').replace(',', '.')
                        else:
                            num_str = num_str.replace(',', '')
                    elif num_str.count(',') == 1 and len(num_str.split(',')[1]) == 2:
                        num_str = num_str.replace(',', '.')
                    elif ',' in num_str:
                        num_str = num_str.replace(',', '')
                    
                    price = float(num_str)
                    
                    if 1 <= price <= 500000:
                        return f"RD${price:,.2f}"
            
            return None
        except:
            return None
    
    def is_potential_category(self, text, href):
        """Determinar si un enlace es una categor√≠a potencial"""
        if not text or not href:
            return False
        
        text_lower = text.lower().strip()
        href_lower = href.lower()
        
        # Filtros negativos
        negative_keywords = [
            'login', 'cuenta', 'carrito', 'buscar', 'ayuda', 'contacto',
            'facebook', 'twitter', 'instagram', 'newsletter' ,'whatsapp'
        ]
        
        if any(neg in text_lower for neg in negative_keywords):
            return False
        
        # Filtros positivos
        positive_url_patterns = [
            'categoria', 'category', 'departamento', 'department',
            'seccion', 'section', '/c/', '/cat/'
        ]
        
        return any(pattern in href_lower for pattern in positive_url_patterns) or self.is_target_category(text)
    
    def is_target_category(self, name):
        """Verificar si coincide con categor√≠as objetivo"""
        if not name:
            return False
        name_lower = name.lower()
        return any(any(keyword in name_lower for keyword in keywords) 
                  for keywords in self.target_categories.values())
    
    def normalize_category_name(self, name):
        """Normalizar nombre de categor√≠a"""
        name_lower = name.lower()
        for category, keywords in self.target_categories.items():
            if any(keyword in name_lower for keyword in keywords):
                return category
        return name.strip().title()
    
    def is_valid_subcategory(self, text, href, main_category):
        """Validar si es una subcategor√≠a v√°lida"""
        if not text or not href or len(text) < 3 or len(text) > 80:
            return False
        
        text_lower = text.lower()
        invalid_terms = [
            'ver todo', 'ver m√°s', 'mostrar todo', 'volver', 'home',
            'p√°gina', 'siguiente', 'anterior', 'filtro', 'casa cuesta'
        ]
        
        return not any(term in text_lower for term in invalid_terms)
    
    def is_valid_product_name(self, name):
        """Validar nombre de producto"""
        if not name or len(name) < 3 or len(name) > 120:
            return False
        
        invalid_terms = [
            'ver m√°s', 'comprar', 'a√±adir', 'carrito', 'login',
            'p√°gina', 'siguiente', 'filtro', 'ordenar'
        ]
        
        name_lower = name.lower()
        return not any(term in name_lower for term in invalid_terms) and \
               bool(re.search(r'[a-zA-Z√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë]', name))
    
    def clean_categories(self, categories):
        """Limpiar y eliminar duplicados de categor√≠as"""
        cleaned = []
        seen_urls = set()
        seen_names = set()
        
        for cat in categories:
            url = cat['url']
            name = cat['name'].lower()
            
            if url not in seen_urls and name not in seen_names:
                seen_urls.add(url)
                seen_names.add(name)
                cleaned.append(cat)
        
        return cleaned
    
    def scrape_complete(self):
        """Proceso completo de scraping con todas las categor√≠as y subcategor√≠as"""
        logger.info(f"üöÄ Iniciando scraping completo - Objetivo: {self.target_products} productos")
        
        if not self.setup_driver():
            return False
        
        try:
            # Paso 1: Obtener categor√≠as principales
            main_categories = self.find_main_categories()
            if not main_categories:
                logger.error("‚ùå No se encontraron categor√≠as principales")
                return False
            
            logger.info(f"üìÇ Encontradas {len(main_categories)} categor√≠as principales")
            
            # Paso 2: Procesar cada categor√≠a principal
            for i, main_cat in enumerate(main_categories, 1):
                if len(self.products_data) >= self.target_products:
                    logger.info(f"üéØ Objetivo de {self.target_products} productos alcanzado")
                    break
                
                logger.info(f"\nüì¶ [{i}/{len(main_categories)}] PROCESANDO: {main_cat['name']}")
                logger.info(f"üîó URL: {main_cat['url']}")
                
                # Extraer productos de la categor√≠a principal
                main_products = self.extract_products_complete(main_cat)
                self.products_data.extend(main_products)
                
                logger.info(f"üìä Productos acumulados: {len(self.products_data)}")
                
                # Paso 3: Buscar y procesar subcategor√≠as
                subcategories = self.find_subcategories(main_cat)
                
                if subcategories:
                    logger.info(f"   üìÅ Procesando {len(subcategories)} subcategor√≠as...")
                    
                    for j, sub_cat in enumerate(subcategories, 1):
                        if len(self.products_data) >= self.target_products:
                            break
                        
                        logger.info(f"      ‚Ü≥ [{j}/{len(subcategories)}] {sub_cat['name']}")
                        
                        # Extraer productos de subcategor√≠a
                        sub_products = self.extract_products_complete(sub_cat)
                        self.products_data.extend(sub_products)
                        
                        time.sleep(1)  # Pausa entre subcategor√≠as
                
                logger.info(f"üìä Total despu√©s de {main_cat['name']}: {len(self.products_data)} productos")
                time.sleep(2)  # Pausa entre categor√≠as principales
            
            logger.info(f"\nüéâ SCRAPING COMPLETADO - Total productos: {len(self.products_data)}")
            return len(self.products_data) > 0
            
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("üîö Driver cerrado")
    
    def save_results(self, filename='jumbo_productos_completo.csv'):
        """Guardar resultados completos en CSV"""
        if not self.products_data:
            logger.warning("‚ö†Ô∏è No hay productos para guardar")
            return
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['nombre', 'precio', 'categoria'])
                writer.writeheader()
                writer.writerows(self.products_data)
            
            logger.info(f"üíæ Datos guardados en {filename}")
            print(f"\n‚úÖ RESULTADOS GUARDADOS EN {filename}")
            print(f"üìä TOTAL DE PRODUCTOS: {len(self.products_data)}")
            
            # Estad√≠sticas detalladas
            main_categories = Counter()
            subcategories = Counter()
            
            for product in self.products_data:
                categoria = product['categoria']
                if '>' in categoria:
                    main_cat, sub_cat = categoria.split('>', 1)
                    main_categories[main_cat.strip()] += 1
                    subcategories[categoria] += 1
                else:
                    main_categories[categoria] += 1
            
            print(f"\nüì¶ PRODUCTOS POR CATEGOR√çA PRINCIPAL:")
            for cat, count in main_categories.most_common():
                print(f"   {cat}: {count} productos")
            
            print(f"\nüîç TOP 10 SUBCATEGOR√çAS:")
            for cat, count in subcategories.most_common(10):
                print(f"   {cat}: {count} productos")
            
            # Ejemplos de productos
            print(f"\nüìù EJEMPLOS DE PRODUCTOS ENCONTRADOS:")
            for i, product in enumerate(self.products_data[:8], 1):
                print(f"   {i}. {product['nombre']} - {product['precio']}")
                print(f"      Categor√≠a: {product['categoria']}")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando CSV: {e}")

def main():
    print("üõí JUMBO.COM.DO - SCRAPER COMPLETO")
    print("=" * 80)
    print("üéØ EXTRACCI√ìN COMPLETA DE PRODUCTOS")
    print("üìÇ Todas las categor√≠as principales y subcategor√≠as")
    print("üìÑ M√∫ltiples p√°ginas por categor√≠a")
    print("üîÑ Sistema anti-duplicados integrado")
    print("=" * 80)
    
    # Configurar objetivo de productos
    try:
        target = int(input("¬øCu√°ntos productos necesitas? (por defecto 2000): ") or "2000")
        if target < 100:
            target = 2000
    except:
        target = 2000
    
    # Configurar modo headless
    headless = True
    respuesta = input("¬øEjecutar en modo visible para monitoreo? (s/N): ").lower()
    if respuesta in ['s', 'si', 's√≠']:
        headless = False
        print("üñ•Ô∏è  Ejecutando en modo visible")
    
    print(f"\nüéØ Objetivo: {target} productos √∫nicos")
    print("üöÄ Iniciando extracci√≥n completa...")
    
    # Ejecutar scraper
    start_time = time.time()
    scraper = JumboCompleteScraper(headless=headless, target_products=target)
    
    if scraper.scrape_complete():
        scraper.save_results()
        elapsed_time = time.time() - start_time
        print(f"\n‚è±Ô∏è  TIEMPO TOTAL: {elapsed_time:.1f} segundos")
        print(f"‚ö° VELOCIDAD: {len(scraper.products_data)/(elapsed_time/60):.1f} productos/minuto")
        print("üéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!")
    else:
        print("‚ùå No se pudieron extraer productos")
    
    print("=" * 80)

if __name__ == "__main__":
    main()